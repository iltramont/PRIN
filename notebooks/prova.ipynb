{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "token = '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' \n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = 'openai-community/gpt2'\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Italy is Rome, and the language of Italy is Italian.\\n\\n'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline('The capital of Italy is', max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[128000,    791,   6864,    315,  15704,    374, 128009, 128009, 128009,\n",
      "         128009, 128009, 128009],\n",
      "        [128000,   9906,     11,    856,    836,    374,  84278,    323,    358,\n",
      "           1097,    264,   3241]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'The capital of Italy is Rome, but the most popular tourist destination in Italy'}],\n",
       " [{'generated_text': 'Hello, my name is Luca and I am a software engineer at a company called TechCorp. I work'}]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"The capital of Italy is\",\n",
    "    'Hello, my name is Luca and I am a software'\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "pprint(tokenized)\n",
    "\n",
    "tokenizer.batch_decode(tokenized['input_ids'])\n",
    "\n",
    "generation_pipeline(prompt, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jan 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon retto<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,   4448,    220,   2366,     20,    271,  54071,  43237,\n",
      "            653,   1812,   4042, 128009, 128006,    882, 128007,    271,  66608,\n",
      "          33920,    653,   2098,  14200,   1891,   5203,    436,   3416,  27564,\n",
      "          33297,   3074,    824,    653,  80585,  13140,    390,    274,   4890,\n",
      "          53979,  23884,   1891,  15756,    461,    453,  15235,   2160,    998,\n",
      "         128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenized, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jan 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Mi dispiace, ma non posso creare un documento di riferimento per una '\n",
      " 'risonanza magnetica (MRI) per un paziente, specialmente se non ho '\n",
      " 'informazioni sulla condizione del paziente, come la sua età, il peso, la '\n",
      " \"presenza di eventuali condizioni preesistenti, l'obiettivo dell'esame, la \"\n",
      " \"posizione del corpo durante l'esame, la tecnica di risonanza magnet\"]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 26 Jan 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti \"\n",
      " 'della retto medio, con una densità di 25 Hounsfield, che potrebbe essere '\n",
      " 'interpretato come un tumore, il che comporta una ipotesi di diagnosi di '\n",
      " 'tumore al colon retto.\\n'\n",
      " '\\n'\n",
      " \"Dopo la risonanza magnetica, il paziente ha presentato un'ipotesi di \"\n",
      " 'diagnosi di tumore al colon retto, in base alla densità del tumore.\\n'\n",
      " '\\n'\n",
      " '**Riferimento**\\n'\n",
      " '\\n'\n",
      " '*   Data: 24/02/2023\\n'\n",
      " '*   Nome: *   *Paziente*\\n'\n",
      " '*   Età: 62 anni\\n'\n",
      " '*   Genere: Maschio\\n'\n",
      " '*   Indirizzo: *   *Via dei Turchi, 123*\\n'\n",
      " '*   Città: *   *Roma*\\n'\n",
      " '*   Provincia: *   *Roma*\\n'\n",
      " '*   Distretto: *   *Roma*\\n'\n",
      " '*   Sede di pratica: *   *Via dei Turchi, 123*\\n'\n",
      " '*  ']\n"
     ]
    }
   ],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti della retto medio, con \"\n",
    "    }\n",
    "]\n",
    "\n",
    "#Remove the generation prompt\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "output = model.generate(tokenized, max_new_tokens=200)\n",
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
