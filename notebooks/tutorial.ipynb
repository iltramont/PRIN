{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    TFAutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "#from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7409a68f4f1e44f4859bfc1a2f94b74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#token = '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'\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Testo'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esame eseguito secondo piani multipli, mediant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze HAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Motivo dell'esame: neoformazione del retto. L'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indagine eseguita mediante sequenze T2 e T1-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tecnica: sequenze FSE, DWI, FSPGR 3D-DISCO pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L'esame è srtato eseguito medinte sequenze FSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Esame eseguito mediante sequenze FSE,GRE e DWI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Testo\n",
       "0  Esame eseguito secondo piani multipli, mediant...\n",
       "1  L'esame è stato eseguito mediante sequenze HAS...\n",
       "2  Motivo dell'esame: neoformazione del retto. L'...\n",
       "3  Indagine eseguita mediante sequenze T2 e T1-di...\n",
       "4  L'esame è stato eseguito mediante sequenze FSE...\n",
       "5  Tecnica: sequenze FSE, DWI, FSPGR 3D-DISCO pri...\n",
       "6  L'esame è srtato eseguito medinte sequenze FSE...\n",
       "7  Esame eseguito mediante sequenze FSE,GRE e DWI...\n",
       "8  L'esame è stato eseguito mediante sequenze FSE...\n",
       "9  L'esame è stato eseguito mediante sequenze FSE..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\lucat\\\\OneDrive\\\\TESI_MAGISTRALE\\\\Referti\\\\Referti non strutturati retto\\\\Referti_puliti.csv'\n",
    "narratives_dataset = load_dataset('csv', data_files=path)\n",
    "print(narratives_dataset)\n",
    "narratives_dataset.set_format('pandas')\n",
    "display(narratives_dataset['train'][:])\n",
    "narratives_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "checkpoint = 'openai-community/gpt2'   \n",
    "\n",
    "generator = pipeline('text-generation', model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'the next avangers movie will be'\n",
    "output = generator(prompt, max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'the next avangers movie will be something more '\n",
      "                    'significant,\" Denton says, in \"Who Smell\". The film will '\n",
      "                    'be directed by Gary Goddard, a former writer, producer '\n",
      "                    'and editor of several major horror magazines.\\n'\n",
      "                    '\\n'\n",
      "                    '\"We\\'re not done'},\n",
      " {'generated_text': 'the next avangers movie will be titled. But, please, stop '\n",
      "                    'making out of anything like that.'},\n",
      " {'generated_text': 'the next avangers movie will be made by CAA/TWC with a '\n",
      "                    'new script (we are still waiting on the actual production '\n",
      "                    'contract to be signed), we will have a live-action movie, '\n",
      "                    'with the script to come in 2019. With'},\n",
      " {'generated_text': 'the next avangers movie will be one with those crazy '\n",
      "                    'flying monsters!\\n'\n",
      "                    '\\n'\n",
      "                    'In other news, there was some awesome action scene with '\n",
      "                    'Dwayne Johnson (The Lego Batman Movie, The Last Jedi, '\n",
      "                    'Star Wars Rogue One and other films) in action'},\n",
      " {'generated_text': \"the next avangers movie will be called 'The Night The \"\n",
      "                    \"Wolf Leaves,' but the team behind Avatar 5 will no doubt \"\n",
      "                    'have a vested interest in getting the film out in time '\n",
      "                    'for this event in their city. While the movie stars '\n",
      "                    'Michael Fassbender'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un piccolo esempio di come funziona un tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Ciao', '▁come', '▁sta', '▁andando', '▁questa', '▁bellissima', '▁sera', 'tona', '?']\n",
      "[111711, 1222, 2339, 206626, 7620, 231424, 8859, 47291, 28]\n",
      "Ciao come sta andando questa bellissima seratona?\n",
      "</s> Ciao come sta andando questa bellissima seratona?\n"
     ]
    }
   ],
   "source": [
    "res = tokenizer('Ciao come sta andando questa bellissima seratona?')\n",
    "tokens = tokenizer.tokenize('Ciao come sta andando questa bellissima seratona?')\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))\n",
    "print(tokenizer.decode(res['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(record):\n",
    "    return tokenizer(record['Testo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a661eebe352d477f9288dd9455325c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "narratives_dataset_tokenized = narratives_dataset.map(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testo</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esame eseguito secondo piani multipli, mediant...</td>\n",
       "      <td>[2, 128, 34634, 213566, 19, 20069, 167370, 129...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze HAS...</td>\n",
       "      <td>[2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Motivo dell'esame: neoformazione del retto. L'...</td>\n",
       "      <td>[2, 60894, 19, 1983, 26, 12, 34634, 13, 24596,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indagine eseguita mediante sequenze T2 e T1-di...</td>\n",
       "      <td>[2, 345, 34842, 108, 213566, 9, 23919, 178600,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "      <td>[2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tecnica: sequenze FSE, DWI, FSPGR 3D-DISCO pri...</td>\n",
       "      <td>[2, 193826, 9, 13, 178600, 756, 333, 6400, 4, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L'esame è srtato eseguito medinte sequenze FSE...</td>\n",
       "      <td>[2, 225, 26, 12, 34634, 468, 28610, 25029, 213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Esame eseguito mediante sequenze FSE,GRE e DWI...</td>\n",
       "      <td>[2, 128, 34634, 213566, 19, 23919, 178600, 756...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "      <td>[2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L'esame è stato eseguito mediante sequenze FSE...</td>\n",
       "      <td>[2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Testo  \\\n",
       "0  Esame eseguito secondo piani multipli, mediant...   \n",
       "1  L'esame è stato eseguito mediante sequenze HAS...   \n",
       "2  Motivo dell'esame: neoformazione del retto. L'...   \n",
       "3  Indagine eseguita mediante sequenze T2 e T1-di...   \n",
       "4  L'esame è stato eseguito mediante sequenze FSE...   \n",
       "5  Tecnica: sequenze FSE, DWI, FSPGR 3D-DISCO pri...   \n",
       "6  L'esame è srtato eseguito medinte sequenze FSE...   \n",
       "7  Esame eseguito mediante sequenze FSE,GRE e DWI...   \n",
       "8  L'esame è stato eseguito mediante sequenze FSE...   \n",
       "9  L'esame è stato eseguito mediante sequenze FSE...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [2, 128, 34634, 213566, 19, 20069, 167370, 129...   \n",
       "1  [2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...   \n",
       "2  [2, 60894, 19, 1983, 26, 12, 34634, 13, 24596,...   \n",
       "3  [2, 345, 34842, 108, 213566, 9, 23919, 178600,...   \n",
       "4  [2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...   \n",
       "5  [2, 193826, 9, 13, 178600, 756, 333, 6400, 4, ...   \n",
       "6  [2, 225, 26, 12, 34634, 468, 28610, 25029, 213...   \n",
       "7  [2, 128, 34634, 213566, 19, 23919, 178600, 756...   \n",
       "8  [2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...   \n",
       "9  [2, 225, 26, 12, 34634, 468, 8277, 213566, 19,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "narratives_dataset_tokenized.set_format('pandas')\n",
    "display(narratives_dataset_tokenized['train'][:])\n",
    "narratives_dataset_tokenized.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Phi-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model microsoft/phi-4 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 567, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.phi3.configuration_phi3.Phi3Config'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m pipe(messages)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model microsoft/phi-4 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lucat\\PythonProjects\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 567, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.phi3.configuration_phi3.Phi3Config'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"microsoft/phi-4\", trust_remote_code=True)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
