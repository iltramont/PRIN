{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iGeniusAI Italia 9B\n",
    "https://huggingface.co/iGeniusAI/Italia-9B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "token = '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' \n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPTNeoXLayer.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miGeniusAI/Italia-9B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m      6\u001b[0m t_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4130\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4124\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   4125\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   4126\u001b[0m     )\n\u001b[0;32m   4128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   4129\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 4130\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   4133\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\iGeniusAI\\Italia-9B-Instruct-v0.1\\e821f1462547cca2a6ff4f7af102d37d9a79fafd\\modeling_italia.py:65\u001b[0m, in \u001b[0;36mItaliaForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_neox \u001b[38;5;241m=\u001b[39m GPTNeoXModel(config)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:1115\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m-> 1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_neox \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoXModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:817\u001b[0m, in \u001b[0;36mGPTNeoXModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mhidden_dropout)\n\u001b[1;32m--> 817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\u001b[43mGPTNeoXLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_eps)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m GPTNeoXRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:817\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mhidden_dropout)\n\u001b[1;32m--> 817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mGPTNeoXLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_eps)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m GPTNeoXRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "\u001b[1;31mTypeError\u001b[0m: GPTNeoXLayer.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model_id = \"iGeniusAI/Italia-9B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "t_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False, \n",
    "    top_p = 0.95, \n",
    "    top_k = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Italy is Rome, which is known for its rich history and'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Il tuo nome è Modello Italia. Tu sei un'intelligenza artificiale, un modello di linguaggio naturale addestrato da iGenius su Leonardo, uno dei supercomputer più potenti al mondo.\"\"\"\n",
    "TEMPERATURE = 0.3\n",
    "MAX_NEW_TOKENS = 250\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Ciao come stai?\"},\n",
    "]\n",
    "\n",
    "conv_template = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "outputs = generation_pipeline(\n",
    "    conv_template,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=TEMPERATURE,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128000,    791,   6864,\n",
      "            315,  15704,    374],\n",
      "        [128000,   9906,     11,    856,    836,    374,  84278,    323,    358,\n",
      "           1097,    264,   3241]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'The capital of Italy is Rome. Rome is a city located in the central'}],\n",
       " [{'generated_text': 'Hello, my name is Luca and I am a software engineer with a passion for artificial intelligence and machine learning'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"The capital of Italy is\",\n",
    "    'Hello, my name is Luca and I am a software'\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "pprint(tokenized)\n",
    "\n",
    "tokenizer.batch_decode(tokenized['input_ids'])\n",
    "\n",
    "generation_pipeline(prompt, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon retto<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2437,  13806,    220,   2366,     20,    271,  54071,  43237,\n",
      "            653,   1812,   4042, 128009, 128006,    882, 128007,    271,  66608,\n",
      "          33920,    653,   2098,  14200,   1891,   5203,    436,   3416,  27564,\n",
      "          33297,   3074,    824,    653,  80585,  13140,    390,    274,   4890,\n",
      "          53979,  23884,   1891,  15756,    461,    453,  15235,   2160,    998,\n",
      "         128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenized, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Non sono un medico e non posso fornire un referto di risonanza magnetica '\n",
      " '(MRI) per un paziente con sospetto caso di tumore al colon retto. La '\n",
      " \"creazione di un referto di risonanza magnetica è un'attività medica che \"\n",
      " 'richiede una valutazione approfondita della condizione del paziente, la sua '\n",
      " 'storia clinica, gli esami di base e la diagnosi preventiva']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti \"\n",
      " \"della retto medio, con un'apertura di circa 2,5 cm, che potrebbe essere \"\n",
      " \"considerato come un'ipertrofia polipica o come un'ipertrofia poliposia, in \"\n",
      " 'base alle informazioni fornite dal paziente.\\n'\n",
      " '\\n'\n",
      " \"La risonanza magnetica (MRI) del paziente ha rilevato un'ipertrofia polipica \"\n",
      " 'nella parte superiore del colon retto, che potrebbe essere considerata come '\n",
      " \"un'ipertrofia poliposia.\\n\"\n",
      " '\\n'\n",
      " 'Riferimento medico:\\n'\n",
      " '\\n'\n",
      " '* Nome del paziente: \\\\[nome del paziente]\\n'\n",
      " \"* Data dell'esame: \\\\[data dell'esame]\\n\"\n",
      " \"* Indicazione dell'esame: Risonanza magnetica per valutazione del colon \"\n",
      " 'retto\\n'\n",
      " \"* Descrizione dell'esame: \\\\[descrizione dell'esame]\\n\"\n",
      " \"* Resultati dell'esame: \\\\[resultati dell'esame]\\n\"\n",
      " '\\n'\n",
      " 'Esempio di risonanza magnet']\n"
     ]
    }
   ],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti della retto medio, con \"\n",
    "    }\n",
    "]\n",
    "\n",
    "#Remove the generation prompt\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "output = model.generate(tokenized, max_new_tokens=200)\n",
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {'role': 'user', 'content': \"Qual è l'università migliore  d'Italia?\"},\n",
    "    {'role': 'assistant', 'content': \"L'università migliore d'Italia è\"}\n",
    "]\n",
    "answer = \"l'Università degli studi di Brescia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è\n"
     ]
    }
   ],
   "source": [
    "chat_template = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    tokenize=False\n",
    ")\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è l'Università degli studi di Brescia<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "print(full_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response_text_tokenized = tokenizer(\n",
    "    full_response_text,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=False\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2437,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,  32129,  11676,    326,  22827,   1986,  24892,\n",
       "          45579,    461,    220,    294,  75750,  19379,     30, 128009, 128006,\n",
       "          78191, 128007,    271,     43,  22827,   1986,  24892,  45579,    461,\n",
       "            294,  75750,  19379,  11676,    326,      6,  65715,  24892,  47669,\n",
       "         106204,   1891,    426,  98612, 128009]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_response_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2437,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  32129,  11676,    326,  22827,   1986,  24892,\n",
      "          45579,    461,    220,    294,  75750,  19379,     30, 128009, 128006,\n",
      "          78191, 128007,    271,     43,  22827,   1986,  24892,  45579,    461,\n",
      "            294,  75750,  19379,  11676,    326,      6,  65715,  24892,  47669,\n",
      "         106204,   1891,    426,  98612]])\n",
      "tensor([[128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,     25,\n",
      "           6790,    220,   2366,     18,    198,  15724,   2696,     25,    220,\n",
      "           2437,  13806,    220,   2366,     20,    271, 128009, 128006,    882,\n",
      "         128007,    271,  32129,  11676,    326,  22827,   1986,  24892,  45579,\n",
      "            461,    220,    294,  75750,  19379,     30, 128009, 128006,  78191,\n",
      "         128007,    271,     43,  22827,   1986,  24892,  45579,    461,    294,\n",
      "          75750,  19379,  11676,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612, 128009]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = full_response_text_tokenized[:, :-1]\n",
    "target_ids = full_response_text_tokenized[:, 1:]\n",
    "print(input_ids)\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'Cut', 'ting', 'ĠKnowledge', 'ĠDate', ':', 'ĠDecember', 'Ġ', '202', '3', 'Ċ', 'Today', 'ĠDate', ':', 'Ġ', '02', 'ĠFeb', 'Ġ', '202', '5', 'ĊĊ', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'Qual', 'ĠÃ¨', 'Ġl', \"'un\", 'ivers', 'itÃł', 'Ġmigli', 'ore', 'Ġ', 'Ġd', \"'It\", 'alia', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'L', \"'un\", 'ivers', 'itÃł', 'Ġmigli', 'ore', 'Ġd', \"'It\", 'alia', 'ĠÃ¨', 'Ġl', \"'\", 'Univers', 'itÃł', 'Ġdegli', 'Ġstudi', 'Ġdi', 'ĠB', 'rescia']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer = tokenizer(\n",
    "    [\" \" + answer + tokenizer.eos_token],\n",
    "    add_special_tokens=False,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=target_ids.shape[1]\n",
    "    )['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009,    326,      6,  65715,  24892,  47669, 106204,\n",
       "           1891,    426,  98612, 128009]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612,   -100]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612, 128009]])\n"
     ]
    }
   ],
   "source": [
    "labels_tokenized_fixed = torch.where(tokenized_answer != tokenizer.pad_token_id, tokenized_answer, -100)\n",
    "print(labels_tokenized_fixed)\n",
    "labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "print(labels_tokenized_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target + tokenizer.eos_token)\n",
    "        for chat, target in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    labels_tokenized = tokenizer(\n",
    "        [\" \" + target + tokenizer.eos_token for target in target_responses],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        max_length=input_ids_tokenized.shape[1]\n",
    "    )['input_ids']\n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "    labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "    attention_mask = input_ids_tokenized != tokenizer.pad_token_id\n",
    "    return {\n",
    "        'input_ids': input_ids_tokenized_left_shifted,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels_tokenized_right_shifted\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_input_output_pair(prompt, [answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è l'Università degli studi di Brescia\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 128256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8643e+00, 2.3100e-01, 3.9315e-02,\n",
       "        5.3392e-04, 2.9884e-01, 3.8662e+00, 4.0301e-03, 2.6740e+00, 7.8209e+00,\n",
       "        1.0148e+01], grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(out.logits, data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è l'università migliore  d'Italia?assistant\n",
      "\n",
      "L'università migliore d'Italia è un argomento molto dibattuto e può variare a seconda delle preferenze e delle priorità delle persone. Tuttavia, ecco alcune delle migliori università italiane, classificate in base alle prestazioni accademiche, al numero di studenti, all'industrializzazione e all'innovazione:\n",
      "\n",
      "1. **Università degli Studi di Milano** (UdM): considerata una delle migliori università italiane, con una reputazione di alta qualità\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = generate_input_output_pair(prompt=[prompt], target_responses=[answer])\n",
    "#optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "#for _ in range(5):\n",
    "#    out = model(input_ids=data['input_ids'])\n",
    "#    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "#    \n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#    optimizer.zero_grad()\n",
    "#    \n",
    "#    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è l'università migliore  d'Italia?assistant\n",
      "\n",
      "L'università migliore d'Italia è\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è la ragazza più swag d'Italia? Dove vive?assistant\n",
      "\n",
      "La ragazza più swag d'Italia è una questione di opinione e di personale giudizio, ma ci sono alcune persone che sono spesso considerate tra le più attraenti e swag d'Italia.\n",
      "\n",
      "Ecco alcune informazioni su alcune delle più belle e attraenti italiane:\n",
      "\n",
      "*   Alessia Marcuzzi: è una modella e cantante italiana nota per le sue opere di moda e la sua carriera musicale.\n",
      "*   Sofia Cavallo: è una modella\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': \"Qual è la ragazza più swag d'Italia? Dove vive?\"},\n",
    "    {'role': 'assistant', 'content': \"La ragazza più swag d'Italia\"}\n",
    "]\n",
    "answer = \"si chiama Alessia e vive a Brescia\"\n",
    "\n",
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8505260944366455\n",
      "loss: 0.6860156059265137\n",
      "loss: 0.486915647983551\n",
      "loss: 0.31546324491500854\n",
      "loss: 0.17947611212730408\n",
      "loss: 0.11014659702777863\n",
      "loss: 0.05896963179111481\n",
      "loss: 0.008632722310721874\n",
      "loss: 0.0016806087223812938\n",
      "loss: 0.0008342156070284545\n"
     ]
    }
   ],
   "source": [
    "data = generate_input_output_pair(prompt=[prompt], target_responses=[answer])\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = model(input_ids=data['input_ids'])\n",
    "    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è la ragazza più swag d'Italia? Dove vive?assistant\n",
      "\n",
      "La ragazza più swag d'Italia si chiama Alessia e vive a Brescia\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
