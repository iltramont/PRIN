{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "token = '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' \n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = 'openai-community/gpt2'\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "\n",
    "generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Italy is Rome, which is known for its rich history and'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline('The capital of Italy is', max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128000,    791,   6864,\n",
      "            315,  15704,    374],\n",
      "        [128000,   9906,     11,    856,    836,    374,  84278,    323,    358,\n",
      "           1097,    264,   3241]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'The capital of Italy is Rome. Rome is a city located in the central'}],\n",
       " [{'generated_text': 'Hello, my name is Luca and I am a software engineer with a passion for artificial intelligence and machine learning'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"The capital of Italy is\",\n",
    "    'Hello, my name is Luca and I am a software'\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "pprint(tokenized)\n",
    "\n",
    "tokenizer.batch_decode(tokenized['input_ids'])\n",
    "\n",
    "generation_pipeline(prompt, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon retto<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2437,  13806,    220,   2366,     20,    271,  54071,  43237,\n",
      "            653,   1812,   4042, 128009, 128006,    882, 128007,    271,  66608,\n",
      "          33920,    653,   2098,  14200,   1891,   5203,    436,   3416,  27564,\n",
      "          33297,   3074,    824,    653,  80585,  13140,    390,    274,   4890,\n",
      "          53979,  23884,   1891,  15756,    461,    453,  15235,   2160,    998,\n",
      "         128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenized, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Non sono un medico e non posso fornire un referto di risonanza magnetica '\n",
      " '(MRI) per un paziente con sospetto caso di tumore al colon retto. La '\n",
      " \"creazione di un referto di risonanza magnetica è un'attività medica che \"\n",
      " 'richiede una valutazione approfondita della condizione del paziente, la sua '\n",
      " 'storia clinica, gli esami di base e la diagnosi preventiva']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 02 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti \"\n",
      " \"della retto medio, con un'apertura di circa 2,5 cm, che potrebbe essere \"\n",
      " \"considerato come un'ipertrofia polipica o come un'ipertrofia poliposia, in \"\n",
      " 'base alle informazioni fornite dal paziente.\\n'\n",
      " '\\n'\n",
      " \"La risonanza magnetica (MRI) del paziente ha rilevato un'ipertrofia polipica \"\n",
      " 'nella parte superiore del colon retto, che potrebbe essere considerata come '\n",
      " \"un'ipertrofia poliposia.\\n\"\n",
      " '\\n'\n",
      " 'Riferimento medico:\\n'\n",
      " '\\n'\n",
      " '* Nome del paziente: \\\\[nome del paziente]\\n'\n",
      " \"* Data dell'esame: \\\\[data dell'esame]\\n\"\n",
      " \"* Indicazione dell'esame: Risonanza magnetica per valutazione del colon \"\n",
      " 'retto\\n'\n",
      " \"* Descrizione dell'esame: \\\\[descrizione dell'esame]\\n\"\n",
      " \"* Resultati dell'esame: \\\\[resultati dell'esame]\\n\"\n",
      " '\\n'\n",
      " 'Esempio di risonanza magnet']\n"
     ]
    }
   ],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti della retto medio, con \"\n",
    "    }\n",
    "]\n",
    "\n",
    "#Remove the generation prompt\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "output = model.generate(tokenized, max_new_tokens=200)\n",
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {'role': 'user', 'content': \"Qual è l'università migliore  d'Italia?\"},\n",
    "    {'role': 'assistant', 'content': \"L'università migliore d'Italia è\"}\n",
    "]\n",
    "answer = \"l'Università degli studi di Brescia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è\n"
     ]
    }
   ],
   "source": [
    "chat_template = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    tokenize=False\n",
    ")\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è l'Università degli studi di Brescia<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "print(full_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response_text_tokenized = tokenizer(\n",
    "    full_response_text,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=False\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2437,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,  32129,  11676,    326,  22827,   1986,  24892,\n",
       "          45579,    461,    220,    294,  75750,  19379,     30, 128009, 128006,\n",
       "          78191, 128007,    271,     43,  22827,   1986,  24892,  45579,    461,\n",
       "            294,  75750,  19379,  11676,    326,      6,  65715,  24892,  47669,\n",
       "         106204,   1891,    426,  98612, 128009]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_response_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2437,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  32129,  11676,    326,  22827,   1986,  24892,\n",
      "          45579,    461,    220,    294,  75750,  19379,     30, 128009, 128006,\n",
      "          78191, 128007,    271,     43,  22827,   1986,  24892,  45579,    461,\n",
      "            294,  75750,  19379,  11676,    326,      6,  65715,  24892,  47669,\n",
      "         106204,   1891,    426,  98612]])\n",
      "tensor([[128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,     25,\n",
      "           6790,    220,   2366,     18,    198,  15724,   2696,     25,    220,\n",
      "           2437,  13806,    220,   2366,     20,    271, 128009, 128006,    882,\n",
      "         128007,    271,  32129,  11676,    326,  22827,   1986,  24892,  45579,\n",
      "            461,    220,    294,  75750,  19379,     30, 128009, 128006,  78191,\n",
      "         128007,    271,     43,  22827,   1986,  24892,  45579,    461,    294,\n",
      "          75750,  19379,  11676,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612, 128009]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = full_response_text_tokenized[:, :-1]\n",
    "target_ids = full_response_text_tokenized[:, 1:]\n",
    "print(input_ids)\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'Cut', 'ting', 'ĠKnowledge', 'ĠDate', ':', 'ĠDecember', 'Ġ', '202', '3', 'Ċ', 'Today', 'ĠDate', ':', 'Ġ', '02', 'ĠFeb', 'Ġ', '202', '5', 'ĊĊ', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'Qual', 'ĠÃ¨', 'Ġl', \"'un\", 'ivers', 'itÃł', 'Ġmigli', 'ore', 'Ġ', 'Ġd', \"'It\", 'alia', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'L', \"'un\", 'ivers', 'itÃł', 'Ġmigli', 'ore', 'Ġd', \"'It\", 'alia', 'ĠÃ¨', 'Ġl', \"'\", 'Univers', 'itÃł', 'Ġdegli', 'Ġstudi', 'Ġdi', 'ĠB', 'rescia']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer = tokenizer(\n",
    "    [\" \" + answer + tokenizer.eos_token],\n",
    "    add_special_tokens=False,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=target_ids.shape[1]\n",
    "    )['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009,    326,      6,  65715,  24892,  47669, 106204,\n",
       "           1891,    426,  98612, 128009]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612,   -100]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612, 128009]])\n"
     ]
    }
   ],
   "source": [
    "labels_tokenized_fixed = torch.where(tokenized_answer != tokenizer.pad_token_id, tokenized_answer, -100)\n",
    "print(labels_tokenized_fixed)\n",
    "labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "print(labels_tokenized_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target + tokenizer.eos_token)\n",
    "        for chat, target in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    labels_tokenized = tokenizer(\n",
    "        [\" \" + target + tokenizer.eos_token for target in target_responses],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        max_length=input_ids_tokenized.shape[1]\n",
    "    )['input_ids']\n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "    labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "    attention_mask = input_ids_tokenized != tokenizer.pad_token_id\n",
    "    return {\n",
    "        'input_ids': input_ids_tokenized_left_shifted,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels_tokenized_right_shifted\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_input_output_pair(prompt, [answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Qual è l'università migliore  d'Italia?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "L'università migliore d'Italia è l'Università degli studi di Brescia\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 128256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8643e+00, 2.3100e-01, 3.9315e-02,\n",
       "        5.3392e-04, 2.9884e-01, 3.8662e+00, 4.0301e-03, 2.6740e+00, 7.8209e+00,\n",
       "        1.0148e+01], grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(out.logits, data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è l'università migliore  d'Italia?assistant\n",
      "\n",
      "L'università migliore d'Italia è un argomento molto dibattuto e può variare a seconda delle preferenze e delle priorità delle persone. Tuttavia, ecco alcune delle migliori università italiane, classificate in base alle prestazioni accademiche, al numero di studenti, all'industrializzazione e all'innovazione:\n",
      "\n",
      "1. **Università degli Studi di Milano** (UdM): considerata una delle migliori università italiane, con una reputazione di alta qualità\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = generate_input_output_pair(prompt=[prompt], target_responses=[answer])\n",
    "#optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "#for _ in range(5):\n",
    "#    out = model(input_ids=data['input_ids'])\n",
    "#    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "#    \n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#    optimizer.zero_grad()\n",
    "#    \n",
    "#    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è l'università migliore  d'Italia?assistant\n",
      "\n",
      "L'università migliore d'Italia è\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è la ragazza più swag d'Italia? Dove vive?assistant\n",
      "\n",
      "La ragazza più swag d'Italia è una questione di opinione e di personale giudizio, ma ci sono alcune persone che sono spesso considerate tra le più attraenti e swag d'Italia.\n",
      "\n",
      "Ecco alcune informazioni su alcune delle più belle e attraenti italiane:\n",
      "\n",
      "*   Alessia Marcuzzi: è una modella e cantante italiana nota per le sue opere di moda e la sua carriera musicale.\n",
      "*   Sofia Cavallo: è una modella\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': \"Qual è la ragazza più swag d'Italia? Dove vive?\"},\n",
    "    {'role': 'assistant', 'content': \"La ragazza più swag d'Italia\"}\n",
    "]\n",
    "answer = \"si chiama Alessia e vive a Brescia\"\n",
    "\n",
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8505260944366455\n",
      "loss: 0.6860156059265137\n",
      "loss: 0.486915647983551\n",
      "loss: 0.31546324491500854\n",
      "loss: 0.17947611212730408\n",
      "loss: 0.11014659702777863\n",
      "loss: 0.05896963179111481\n",
      "loss: 0.008632722310721874\n",
      "loss: 0.0016806087223812938\n",
      "loss: 0.0008342156070284545\n"
     ]
    }
   ],
   "source": [
    "data = generate_input_output_pair(prompt=[prompt], target_responses=[answer])\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = model(input_ids=data['input_ids'])\n",
    "    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è la ragazza più swag d'Italia? Dove vive?assistant\n",
      "\n",
      "La ragazza più swag d'Italia si chiama Alessia e vive a Brescia\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
