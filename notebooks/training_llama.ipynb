{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "token = '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' \n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "\n",
    "generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The capital of Italy is Rome. Rome is a major city in Italy known for its rich history, architecture, and art. The city is home to many famous landmarks such as the Colosseum, the Pantheon, and the Vatican City. Rome is also known for its delicious food and wine. Visitors to Rome often spend their days exploring the city's many historical sites and enjoying the local cuisine. The city has a population of over 2.8 million people and is a popular destination for tourists from all over the world.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline('The capital of Italy is', max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128000,    791,   6864,\n",
      "            315,  15704,    374],\n",
      "        [128000,   9906,     11,    856,    836,    374,  84278,    323,    358,\n",
      "           1097,    264,   3241]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'The capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome.\\nThe capital of Italy is Rome'}],\n",
       " [{'generated_text': \"Hello, my name is Luca and I am a software engineer with a passion for coding. I've been working on a project to build a chatbot that can understand and respond to user queries. I've been using Python as my primary language for this project and I've been experimenting with different libraries and frameworks\"}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"The capital of Italy is\",\n",
    "    'Hello, my name is Luca and I am a software'\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "pprint(tokenized)\n",
    "\n",
    "tokenizer.batch_decode(tokenized['input_ids'])\n",
    "\n",
    "generation_pipeline(prompt, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 10 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon retto<|eot_id|>')\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    605,  13806,    220,   2366,     20,    271,  54071,  43237,\n",
      "            653,   1812,   4042, 128009, 128006,    882, 128007,    271,  66608,\n",
      "          33920,    653,   2098,  14200,   1891,   5203,    436,   3416,  27564,\n",
      "          33297,   3074,    824,    653,  80585,  13140,    390,    274,   4890,\n",
      "          53979,  23884,   1891,  15756,    461,    453,  15235,   2160,    998,\n",
      "         128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenized, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 10 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " '**Confidenza medica**\\n'\n",
      " '\\n'\n",
      " '**Data:** 10 Febbraio 2025\\n'\n",
      " '\\n'\n",
      " '**Nome e cognome:** \\\\[Nome del paziente]\\n'\n",
      " '\\n'\n",
      " '**Indirizzo:** \\\\[Indirizzo del paziente]\\n'\n",
      " '\\n'\n",
      " '**Città e stato:** \\\\[Città e stato del paziente]\\n'\n",
      " '\\n'\n",
      " '**Ora:** \\\\[Ora del test]\\n'\n",
      " '\\n'\n",
      " '**Teste medico:** \\\\[Nome e cognome del medico]\\n'\n",
      " '\\n'\n",
      " '**Descrizione del']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Cutting Knowledge Date: December 2023\\n'\n",
      " 'Today Date: 10 Feb 2025\\n'\n",
      " '\\n'\n",
      " 'Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto '\n",
      " 'caso di tumore al colon '\n",
      " 'retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
      " '\\n'\n",
      " \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti \"\n",
      " \"della retto medio, con un'area di iperlatticazione centrale, evidenziata \"\n",
      " \"dalla segnatura di un'espansione anormale.\\n\"\n",
      " '\\n'\n",
      " 'La risonanza magnetica (MRI) effettuata il 15/02/2023, evidenziata con '\n",
      " \"un'immagine T1-weighted, mostra una presenza di un'area iperlattica con una \"\n",
      " 'dimensione di 4,5 cm x 3,8 cm, localizzata nella parte inferiore della '\n",
      " 'colonna rettale, con una dimensione di 3,2 cm x 2,8 cm.\\n'\n",
      " '\\n'\n",
      " \"La segnatura di un'espansione anormale è evidenziata dalla segnatura di \"\n",
      " \"un'espansione anormale, con un'area di iperlatticazione centrale, \"\n",
      " \"evidenziata dalla segnatura di un'espansione anormale.\\n\"\n",
      " '\\n'\n",
      " \"La presenza di un'area ip\"]\n"
     ]
    }
   ],
   "source": [
    "prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Si documenta la presenza di un'ispessimento circonferenziale delle pareti della retto medio, con \"\n",
    "    }\n",
    "]\n",
    "\n",
    "#Remove the generation prompt\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_gneration_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "output = model.generate(tokenized, max_new_tokens=200)\n",
    "pprint(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {'role': 'user', 'content': \"Qual è l'università migliore  d'Italia?\"},\n",
    "    {'role': 'assistant', 'content': \"L'università migliore d'Italia è\"}\n",
    "]\n",
    "answer = \"l'Università degli studi di Brescia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    continue_final_message=True,\n",
    "    tokenize=False\n",
    ")\n",
    "#print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "#print(full_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response_text_tokenized = tokenizer(\n",
    "    full_response_text,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=False\n",
    ")['input_ids']\n",
    "#full_response_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = full_response_text_tokenized[:, :-1]\n",
    "target_ids = full_response_text_tokenized[:, 1:]\n",
    "#print(input_ids)\n",
    "#print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer = tokenizer(\n",
    "    [\" \" + answer + tokenizer.eos_token],\n",
    "    add_special_tokens=False,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=target_ids.shape[1]\n",
    "    )['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009,    326,      6,  65715,  24892,  47669, 106204,\n",
       "           1891,    426,  98612, 128009]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612,   -100]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,    326,      6,  65715,  24892,  47669, 106204,\n",
      "           1891,    426,  98612, 128009]])\n"
     ]
    }
   ],
   "source": [
    "labels_tokenized_fixed = torch.where(tokenized_answer != tokenizer.pad_token_id, tokenized_answer, -100)\n",
    "print(labels_tokenized_fixed)\n",
    "labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "print(labels_tokenized_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carica referti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Intestazione', 'Esame', 'TESTO:', 'TECNICA:',\n",
      "       'QUALITA' DELLE IMMAGINI: ', 'INDICAZIONI: ', 'CONFRONTO: ', 'RETTO:',\n",
      "       'Posizione: ', 'Estensione cranio-caudale: ',\n",
      "       'Estensione circonferenziale: ', 'Infiltrazione sfintere: ',\n",
      "       'Coinvolgimento fascia mesorettale:',\n",
      "       'Localizzazione della distanza minima tra tumore e MRF (mesorectal fascia): ',\n",
      "       'Linfonodi mesorettali: ', 'Depositi tumorali: ', 'Altri reperti: ',\n",
      "       'CONCLUSIONI: ',\n",
      "       'Note cliniche GEMELLI: mi piacerebbe conoscere lo stato EMVI (extramural vascular invasion), se ci sono linfonodi extra mesorettali positivi (inguinali, otturatori, iliaci interni ed esterni, iliaci comuni)'],\n",
      "      dtype='object', name='Caso')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_path = 'C:\\\\Users\\\\lucat\\\\OneDrive\\\\TESI_MAGISTRALE\\\\Referti\\Referti non strutturati retto\\\\Referti con annotazioni.xlsx'\n",
    "data = pd.read_excel(data_path, index_col=0)\n",
    "data = data.T\n",
    "#narratives = data.iloc[:, 2]\n",
    "pprint(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Testo\n",
      "0  Esame eseguito secondo piani multipli, mediant...\n",
      "1  L'esame è stato eseguito mediante sequenze HAS...\n",
      "2  Motivo dell'esame: neoformazione del retto. L'...\n",
      "3  Indagine eseguita mediante sequenze T2 e T1-di...\n",
      "4  L'esame è stato eseguito mediante sequenze FSE...\n",
      "5  Tecnica: sequenze FSE, DWI, FSPGR 3D-DISCO pri...\n",
      "6  L'esame è srtato eseguito medinte sequenze FSE...\n",
      "7  Esame eseguito mediante sequenze FSE,GRE e DWI...\n",
      "8  L'esame è stato eseguito mediante sequenze FSE...\n",
      "9  L'esame è stato eseguito mediante sequenze FSE...\n"
     ]
    }
   ],
   "source": [
    "# Select Narratives (testo dei referti)\n",
    "narratives = data['TESTO:']\n",
    "narratives.reset_index(drop=True, inplace=True)\n",
    "data = pd.DataFrame(narratives)\n",
    "data.rename(columns={'TESTO:': 'Testo'}, inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genera prompt per allenamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt =[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Tu sei un medico'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "for text in data['Testo']:\n",
    "    prompts.append(basic_prompt)\n",
    "    \n",
    "target_responses = data['Testo'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompts, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompts, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = [\n",
    "        #(chat + \" \" + target + tokenizer.eos_token)\n",
    "        (chat + target + tokenizer.eos_token)\n",
    "        for chat, target in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors='pt', add_special_tokens=False, padding=True)['input_ids']\n",
    "    \n",
    "    labels_tokenized = tokenizer(\n",
    "        [\" \" + target + tokenizer.eos_token for target in target_responses],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        max_length=input_ids_tokenized.shape[1]\n",
    "    )['input_ids']\n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "    labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "    attention_mask = input_ids_tokenized != tokenizer.pad_token_id\n",
    "    return {\n",
    "        'input_ids': input_ids_tokenized_left_shifted,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels_tokenized_right_shifted\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128009, 128009, 128009,  ...,   7546,   8115,    570],\n",
      "        [128009, 128009, 128009,  ...,    328,     17,     13],\n",
      "        [128009, 128009, 128009,  ...,    992,     72,     13],\n",
      "        ...,\n",
      "        [128000, 128006,   9125,  ...,   5683,   1604,     13],\n",
      "        [128009, 128009, 128009,  ...,   1891,   4224,     13],\n",
      "        [128009, 128009, 128009,  ...,     18,    452,     16]])\n",
      "tensor([[False, False, False,  ...,  True,  True, False],\n",
      "        [False, False, False,  ...,  True,  True, False],\n",
      "        [False, False, False,  ...,  True,  True, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ...,  True,  True, False],\n",
      "        [False, False, False,  ...,  True,  True, False],\n",
      "        [False, False, False,  ...,  True,  True, False]])\n",
      "tensor([[  -100,   -100,   -100,  ...,   8115,    570, 128009],\n",
      "        [  -100,   -100,   -100,  ...,     17,     13, 128009],\n",
      "        [  -100,   -100,   -100,  ...,     72,     13, 128009],\n",
      "        ...,\n",
      "        [  -100,   -100,   -100,  ...,   1604,     13, 128009],\n",
      "        [  -100,   -100,   -100,  ...,   4224,     13, 128009],\n",
      "        [  -100,   -100,   -100,  ...,    452,     16, 128009]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = generate_input_output_pair(prompts, target_responses)\n",
    "print(data['input_ids'])\n",
    "print(data['attention_mask'])\n",
    "print(data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "Tu sei un medico<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon retto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|>Esame eseguito secondo piani multipli, mediante sequenze T1- e T2-dipendenti e di Diffusione, completato mediante sono state mezzo di contrasto (Prohance, 16 mL). A circa 2,5 cm dalla giunzione ano-rettale si documenta formazione espansiva solida semianulare che si estende da ore 2 ad ore 7 per una estensione cranio-caudale di 5 cm. Essa presenta fini spiculature che si estendono nel tessuto adiposo pararettale per un'estensione inferiore ai 5 mm. Sono presenti tre linfonodi con aspetto tondeggiante, segnale irregolare e margini mal definiti; uno di questi, in sede craniale, giunge a contatto con la fascia mesorettale (vedi immagii principali). Si documenta sospetta esntesione della vascolarizzazione al versante alterale di sinistra (vedi immagii principali).\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 660, 128256])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[1;34m(logits, labels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate_loss\u001b[39m(logits, labels):\n\u001b[0;32m      2\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     cross_entropy_loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_loss\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "#calculate_loss(out.logits, data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "Tu sei un medicouser\n",
      "\n",
      "Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon rettoassistant\n",
      "\n",
      "assistant\n",
      "\n",
      "**REFERTO RISONANZA MAGNETICA**\n",
      "\n",
      "**Data:** \\[Data dell'esame]\n",
      "\n",
      "**Patient:** \\[Nome e cognome del paziente, in questo caso \"Paziente A\"]\n",
      "\n",
      "**Indicazione terapeutica:** Risonanza magnetica pervalore\n",
      "\n",
      "**Sintesi del caso:**\n",
      "\n",
      "Il paziente A, \\[nome e cognome], \\[età], \\[indirizzo], \\[numero\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompts,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = generate_input_output_pair(prompt=[prompt], target_responses=[answer])\n",
    "#optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "#for _ in range(5):\n",
    "#    out = model(input_ids=data['input_ids'])\n",
    "#    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "#    \n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#    optimizer.zero_grad()\n",
    "#    \n",
    "#    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_tokenized = tokenizer.apply_chat_template(\n",
    "#    prompts,\n",
    "#    continue_final_message=True,\n",
    "#    return_tensors='pt'\n",
    "#)\n",
    "#test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "#print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "Tu sei un medicouser\n",
      "\n",
      "Scrivi un referto di una risonanza magnetica per un paziente con sospetto caso di tumore al colon rettoassistant\n",
      "\n",
      "assistant\n",
      "\n",
      "**Referto di Risonanza Magnetica**\n",
      "\n",
      "**Data di esecuzione:** 10 Febbraio 2025\n",
      "\n",
      "**Paziente:** \\[Nome e cognome del paziente non fornito per motivi di privacy]\n",
      "\n",
      "**Assunto:** Sospetto caso di tumore al colon retto\n",
      "\n",
      "**Descrizione del paziente:**\n",
      "\n",
      "- \\[Nome e cognome del paziente]\n",
      "- \\[Età del paziente]\n",
      "- \\[Sexo del paziente]\n",
      "- \\[Stato di salute del paziente]\n",
      "- \\[Data di nascita del paziente]\n",
      "\n",
      "**Simboli risonanza magnetica:**\n",
      "\n",
      "- **Tabelle di risultati:**\n",
      "  - Tabello 1: Tabelle dei risultati della risonanza magnetica del colon retto\n",
      "  - Tabello 2: Tabelle dei risultati della risonanza magnetica della regione retrocelessale\n",
      "- **Gruppi di contrasto:**\n",
      "  - Gruppo A: Tabelle dei risultati della risonanza magnetica del colon retto\n",
      "  - Gruppo B: Tabelle dei risultati della risonanza magnetica della regione retrocelessale\n",
      "  - Gruppo C: Tabelle dei risultati della risonanza magnetica della regione retrocelessale\n",
      "  - Gruppo D: Tabelle dei risultati della risonanza magnetica del colon retto\n",
      "\n",
      "**Risultati della risonanza magnetica:**\n",
      "\n",
      "- **Tabelle dei risultati della risonanza magnetica del colon retto:**\n",
      "  - \\[Numero] \\[Tabelle dei risultati della risonanza magnetica del colon retto]\n",
      "  - \\[Tabelle dei risultati della risonanza magnetica della regione retrocelessale]\n",
      "- **Gruppi di contrasto:**\n",
      "  - \\[Gruppo A] \\[Tabelle dei risultati della risonanza magnetica del colon retto]\n",
      "  - \\[Gruppo B] \\[Tabelle dei risultati della risonanza magnetica della regione retrocelessale]\n",
      "  - \\[Gruppo C] \\[Tabelle dei risultati della risonanza magnetica della regione retrocelessale]\n",
      "  - \\[Gruppo D] \\[Tabelle dei risultati della risonanza magnetica del colon retto]\n",
      "\n",
      "**Conclusione:**\n",
      "\n",
      "La risonanza magnetica ha rilevato alcune anomali nella struttura e nell'attività del colon retto, che potrebbero indicare la presenza di un tumore. Tuttavia, è importante notare che i risultati non sono definitivi e richiedono ulteriori esami e test per una valutazione più approfondita.\n",
      "\n",
      "**Recomandazioni:**\n",
      "\n",
      "- \\[Recomandazioni specifiche per il paziente, ad esempio l'invio della cira per un'ecografia, l'invio della cira per una biopsia, ecc.]\n",
      "\n",
      "**Nota:** Questo referto è un esempio e potrebbe essere modificato in base alle specifiche esigenze del paziente e alle linee guida cliniche in vigore.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompts,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=1000)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = generate_input_output_pair(prompts, target_responses)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = model(input_ids=data['input_ids'])\n",
    "    loss = calculate_loss(out.logits, data['labels']).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('loss:', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Qual è la ragazza più swag d'Italia? Dove vive?assistant\n",
      "\n",
      "La ragazza più swag d'Italia si chiama Alessia e vive a Brescia\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(\n",
    "    prompts,\n",
    "    continue_final_message=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
