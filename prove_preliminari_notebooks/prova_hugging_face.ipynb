{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    'I hate it!',\n",
    "    'I love it!',\n",
    "    'what a shame'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'label': 'NEGATIVE', 'score': 0.9995840191841125}, {'label': 'POSITIVE', 'score': 0.9998781681060791}, {'label': 'NEGATIVE', 'score': 0.9997525811195374}]\n"
     ]
    }
   ],
   "source": [
    "results = classifier(data)\n",
    "print(type(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(data, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "tf.Tensor(\n",
      "[[ 101 1045 5223 2009  999  102]\n",
      " [ 101 1045 2293 2009  999  102]\n",
      " [ 101 2054 1037 9467  102    0]], shape=(3, 6), dtype=int32)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "attention_mask\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0]], shape=(3, 6), dtype=int32)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in inputs.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print(100*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4.322109  -3.462332 ]\n",
      " [-4.3268723  4.6855288]\n",
      " [ 4.5768104 -3.7275047]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.99584019e-01 4.15986753e-04]\n",
      " [1.21873934e-04 9.99878168e-01]\n",
      " [9.99752581e-01 2.47385411e-04]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "predictions_label = tf.math.argmax(predictions, axis=-1)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate it! \t---> NEGATIVE\n",
      "I love it! \t---> POSITIVE\n",
      "what a shame \t---> NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "for f, l in zip(data, predictions_label.numpy()):\n",
    "    print(f, '\\t--->', model.config.id2label[l.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartModel.\n",
      "\n",
      "All the weights of TFBartModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.modeling_tf_bart.TFBartModel'>\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = TFAutoModel.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = TFAutoModel.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']\n",
      "[2292, 1005, 1055, 3046, 2000, 19204, 4697, 999]\n",
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n",
      "[CLS] let ' s try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Let's try to tokenize!\"\n",
    "tokens = tokenizer.tokenize(phrase)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(tokens)\n",
    "print(input_ids)\n",
    "print(final_inputs['input_ids'])\n",
    "print(tokenizer.decode(final_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] let ' s try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(phrase)\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\t[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n",
      "token_type_ids:\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask:\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for i, j in inputs.items():\n",
    "    print(f'{i}:\\t{j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inputs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate it! [1045, 5223, 2009, 999]\n",
      "I love it! [1045, 2293, 2009, 999]\n",
      "what a shame [2054, 1037, 9467]\n"
     ]
    }
   ],
   "source": [
    "sentences = data\n",
    "\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i], ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not possible, no padding!\n",
    "#tf.constant(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[101, 1045, 5223, 2009, 999, 102], [101, 1045, 2293, 2009, 999, 102], [101, 2054, 1037, 9467, 102]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "input_ids [[101, 1045, 5223, 2009, 999, 102], [101, 1045, 2293, 2009, 999, 102], [101, 2054, 1037, 9467, 102, 0]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "no_padding = tokenizer(sentences, padding=False)\n",
    "yes_padding = tokenizer(sentences, padding=True)\n",
    "for k, v in no_padding.items():\n",
    "    print(k, v)\n",
    "print(100*'-')\n",
    "for k, v in yes_padding.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[101, 1045, 5223, 2009, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2293, 2009, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 1037, 9467, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Differtent kind of padding\n",
    "max_len_padding = tokenizer(sentences, padding='max_length', max_length=15)\n",
    "\n",
    "for k, v in max_len_padding.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "{'idx': 6,\n",
      " 'label': 0,\n",
      " 'sentence1': 'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , '\n",
      "              'closing at 1,520.15 on Friday .',\n",
      " 'sentence2': 'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or '\n",
      "              '2.04 percent , to 1,520.15 .'}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'])\n",
    "pprint(raw_datasets['train'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value(dtype='int32', id=None),\n",
      " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
      " 'sentence1': Value(dtype='string', id=None),\n",
      " 'sentence2': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(raw_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': [0, 1, 2, 3, 4, 5],\n",
      " 'label': [1, 0, 1, 0, 1, 1],\n",
      " 'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , '\n",
      "               'of deliberately distorting his evidence .',\n",
      "               \"Yucaipa owned Dominick 's before selling the chain to Safeway \"\n",
      "               'in 1998 for $ 2.5 billion .',\n",
      "               'They had published an advertisement on the Internet on June 10 '\n",
      "               ', offering the cargo for sale , he added .',\n",
      "               'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at '\n",
      "               'A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
      "               'The stock rose $ 2.11 , or about 11 percent , to close Friday '\n",
      "               'at $ 21.51 on the New York Stock Exchange .',\n",
      "               'Revenue in the first quarter of the year dropped 15 percent '\n",
      "               'from the same period a year earlier .'],\n",
      " 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his '\n",
      "               'brother of deliberately distorting his evidence .',\n",
      "               \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold \"\n",
      "               'it to Safeway for $ 1.8 billion in 1998 .',\n",
      "               \"On June 10 , the ship 's owners had published an advertisement \"\n",
      "               'on the Internet , offering the explosives for sale .',\n",
      "               'Tab shares jumped 20 cents , or 4.6 % , to set a record '\n",
      "               'closing high at A $ 4.57 .',\n",
      "               'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on '\n",
      "               'the New York Stock Exchange on Friday .',\n",
      "               \"With the scandal hanging over Stewart 's company , revenue the \"\n",
      "               'first quarter of the year dropped 15 percent from the same '\n",
      "               'period a year earlier .']}\n"
     ]
    }
   ],
   "source": [
    "pprint(raw_datasets['train'][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['sentence1'], example['sentence2'], padding='max_length', truncation=True, max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 408/408 [00:00<00:00, 3481.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets = tokenized_datasets.with_format('tensorflow')\n",
    "print(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# To generate a short example of the dataset\n",
    "small_train_dataset = tokenized_datasets['train'].select(range(100))\n",
    "print(small_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I hate it!', 'I love it!', 'what a shame']\n"
     ]
    }
   ],
   "source": [
    "pprint(sentences)\n",
    "batch = tokenizer(sentences, padding=True, truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 5223, 2009,  999,  102],\n",
       "       [ 101, 1045, 2293, 2009,  999,  102],\n",
       "       [ 101, 2054, 1037, 9467,  102,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'My name isa Luca'\n",
    "b = 'I am a software developer'\n",
    "c = 'going to the cinema'\n",
    "d = 'this movie is great'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               2026,\n",
      "               2171,\n",
      "               18061,\n",
      "               15604,\n",
      "               102,\n",
      "               1045,\n",
      "               2572,\n",
      "               1037,\n",
      "               4007,\n",
      "               9722,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "pair = tokenizer(a, b)\n",
    "pprint(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>,\n",
      " 'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[  101,  2026,  2171, 18061, 15604,   102,  1045,  2572,  1037,\n",
      "         4007,  9722,   102],\n",
      "       [  101,  2183,  2000,  1996,  5988,   102,  2023,  3185,  2003,\n",
      "         2307,   102,     0]], dtype=int32)>,\n",
      " 'token_type_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer([a, c], [b, d], padding=True, return_tensors='tf')\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.38012093, -1.1027563 ],\n",
       "       [-0.35463804, -0.94542867]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning and transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucat\\PythonRepositories\\PRIN\\.venv\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'bert-base-cased'\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_252 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108311810 (413.18 MB)\n",
      "Trainable params: 108311810 (413.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
